{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b1d6b7-d9ff-40fe-94ca-fa3054d1f74f",
   "metadata": {},
   "source": [
    "## Spark on GPU-enabled Kubernetes\n",
    "\n",
    "Build image to run and submit Apache Spark applications on Kubernetes. Steps include downloading files from Nvidia and Spark into a local `src/` folder. In this example, no operators are required.\n",
    "\n",
    "## Google Cloud services and resources:\n",
    "\n",
    "- `Vertex AI`\n",
    "- `Artifact Registry`\n",
    "- `Cloud Storage`\n",
    "- `Kubernetes Engine`\n",
    "- `Compute Engine`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d91aaf-04e0-4268-ab8a-c15792ad6c00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spark Docker Image preparation\n",
    "\n",
    "Set up required parameters.\n",
    "\n",
    "Container Image:\n",
    "- `REPO_NAME`: version or tag of the Docker image. Default set as `latest`\n",
    "- `REPO_NAME`: The name of the Artifact Registry repository that will store the compiled pipeline file\n",
    "- `JOB_IMAGE_ID`: The name of the image that will be used to run spark jobs on Kubernetes. The full image name: `<REGION>-docker.pkg.dev/<PROJECT_ID>/<REPO_NAME>/<JOB_IMAGE_ID>:<VERSION>`\n",
    "- `BASE_IMAGE_ID`: The name of the image that will be used to submit jobs using Vertex AI. The full image name: `<REGION>-docker.pkg.dev/<PROJECT_ID>/<REPO_NAME>/<BASE_IMAGE_ID>:<VERSION>`\n",
    "<br>\n",
    "\n",
    "Custom Job and Pipeline:\n",
    "- `SERVICE_ACCOUNT`: The service account to use to run custom jobs and pipeline\n",
    "\n",
    "The final local `/src` folder will include the following: Dockerfile.cuda, spark (folder), getGpusResources.sh, rapids-4-spark_2.12-23.02.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfb479-5472-405f-b4e2-113361a82f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "VERSION=\"latest\"\n",
    "REPO_NAME=\"gke-mlops-pilot-docker\" # @param {type:\"string\"}\n",
    "JOB_IMAGE_ID=\"spark-gke\" # @param {type:\"string\"}\n",
    "BASE_IMAGE_ID = \"component-base\" # @param {type:\"string\"}\n",
    "\n",
    "# Vertex Custom Job parameters\n",
    "SERVICE_ACCOUNT=\"757654702990-compute@developer.gserviceaccount.com\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfead4-b4af-4ce0-bdad-aa2137175ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/Dockerfile.cuda\n",
    "\n",
    "#\n",
    "# Copyright (c) 2020-2023, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "FROM nvidia/cuda:11.8.0-devel-ubuntu20.04\n",
    "ARG spark_uid=185\n",
    "\n",
    "# https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212771\n",
    "RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n",
    "\n",
    "# Install java dependencies\n",
    "ENV DEBIAN_FRONTEND=\"noninteractive\"\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends openjdk-8-jdk openjdk-8-jre\n",
    "ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "ENV PATH $PATH:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
    "\n",
    "# Before building the docker image, first either download Apache Spark 3.1+ from\n",
    "# http://spark.apache.org/downloads.html or build and make a Spark distribution following the\n",
    "# instructions in http://spark.apache.org/docs/3.1.2/building-spark.html (see\n",
    "# https://nvidia.github.io/spark-rapids/docs/download.html for other supported versions).  If this\n",
    "# docker file is being used in the context of building your images from a Spark distribution, the\n",
    "# docker build command should be invoked from the top level directory of the Spark\n",
    "# distribution. E.g.: docker build -t spark:3.1.2 -f kubernetes/dockerfiles/spark/Dockerfile .\n",
    "\n",
    "RUN set -ex && \\\n",
    "    ln -s /lib /lib64 && \\\n",
    "    mkdir -p /opt/spark && \\\n",
    "    mkdir -p /opt/spark/jars && \\\n",
    "    mkdir -p /opt/spark/examples && \\\n",
    "    mkdir -p /opt/spark/work-dir && \\\n",
    "    mkdir -p /opt/sparkRapidsPlugin && \\\n",
    "    touch /opt/spark/RELEASE && \\\n",
    "    rm /bin/sh && \\\n",
    "    ln -sv /bin/bash /bin/sh && \\\n",
    "    echo \"auth required pam_wheel.so use_uid\" >> /etc/pam.d/su && \\\n",
    "    chgrp root /etc/passwd && chmod ug+rw /etc/passwd\n",
    "\n",
    "COPY spark/jars /opt/spark/jars\n",
    "COPY spark/bin /opt/spark/bin\n",
    "COPY spark/sbin /opt/spark/sbin\n",
    "COPY spark/kubernetes/dockerfiles/spark/entrypoint.sh /opt/\n",
    "COPY spark/examples /opt/spark/examples\n",
    "COPY spark/kubernetes/tests /opt/spark/tests\n",
    "COPY spark/data /opt/spark/data\n",
    "\n",
    "COPY rapids-4-spark_2.12-23.10.0.jar /opt/sparkRapidsPlugin\n",
    "COPY getGpusResources.sh /opt/sparkRapidsPlugin\n",
    "\n",
    "RUN mkdir /opt/spark/python\n",
    "# TODO: Investigate running both pip and pip3 via virtualenvs\n",
    "RUN apt-get update && \\\n",
    "    apt install -y python wget && wget https://bootstrap.pypa.io/pip/2.7/get-pip.py && python get-pip.py && \\\n",
    "    apt install -y python3 python3-pip && \\\n",
    "    # We remove ensurepip since it adds no functionality since pip is\n",
    "    # installed on the image and it just takes up 1.6MB on the image\n",
    "    rm -r /usr/lib/python*/ensurepip && \\\n",
    "    pip install --upgrade pip setuptools && \\\n",
    "    # You may install with python3 packages by using pip3.6\n",
    "    # Removed the .cache to save space\n",
    "    rm -r /root/.cache && rm -rf /var/cache/apt/*\n",
    "\n",
    "COPY spark/python/pyspark /opt/spark/python/pyspark\n",
    "COPY spark/python/lib /opt/spark/python/lib\n",
    "\n",
    "ENV SPARK_HOME /opt/spark\n",
    "\n",
    "WORKDIR /opt/spark/work-dir\n",
    "RUN chmod g+w /opt/spark/work-dir\n",
    "\n",
    "ENV TINI_VERSION v0.18.0\n",
    "ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /usr/bin/tini\n",
    "RUN chmod +rx /usr/bin/tini\n",
    "\n",
    "ENTRYPOINT [ \"/opt/entrypoint.sh\" ]\n",
    "\n",
    "# Specify the User that the actual main process will run as\n",
    "USER ${spark_uid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefabcd0-334c-45a4-b80b-b16b6cdc3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest Spark and shell scripts locally\n",
    "! wget -P ./src https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# Un-tar and rename to spark\n",
    "! tar -xzf ./src/spark-3.5.0-bin-hadoop3.tgz -C ./src && \\\n",
    "  mv ./src/spark-3.5.0-bin-hadoop3 ./src/spark && \\\n",
    "  rm ./src/spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# Download GPU resources\n",
    "! wget -P ./src https://github.com/apache/spark/blob/master/examples/src/main/scripts/getGpusResources.sh\n",
    "\n",
    "# download Rapids Jar file\n",
    "! wget -P ./src https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/23.10.0/rapids-4-spark_2.12-23.10.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c5bc2-8353-4e30-b268-11d430766c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push image to reigstry\n",
    "! docker build ./src -f ./src/Dockerfile.cuda -t {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}\n",
    "! gcloud auth configure-docker us-central1-docker.pkg.dev --quiet\n",
    "! docker push {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb91eea-834a-418c-ba4b-c72753ab73c2",
   "metadata": {},
   "source": [
    "## Custom Jobs\n",
    "\n",
    "Submit Spark applications to Kubernetes using [Custom Jobs and Worker Pool Specs](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d84fb9-f80d-4e97-9687-a54bff1da6b5",
   "metadata": {},
   "source": [
    "### Create Custom Job base-image\n",
    "\n",
    "This image includes Spark, kubectl, and the gcloud SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff017b97-f965-4807-acb5-d61608358f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/Dockerfile\n",
    "\n",
    "FROM google/cloud-sdk:latest\n",
    "\n",
    "# Install kubectl\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y apt-transport-https ca-certificates curl && \\\n",
    "    curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \\\n",
    "    echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list && \\\n",
    "    apt-get update && \\\n",
    "    apt-get install -y kubectl\n",
    "\n",
    "# Install pip\n",
    "RUN apt-get install -y python3-pip\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN apt-get install -y cmake\n",
    "RUN apt-get install -y net-tools\n",
    "\n",
    "# Download and extract Apache Spark\n",
    "RUN curl -O https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz && \\\n",
    "    tar -xzf spark-3.5.0-bin-hadoop3.tgz && \\\n",
    "    mv spark-3.5.0-bin-hadoop3 spark\n",
    "\n",
    "# Change working directory to spark\n",
    "WORKDIR /spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f6b88-8c80-4dfb-beab-c09b7c9c6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push image to reigstry\n",
    "\n",
    "! docker build ./src -f ./src/Dockerfile -t {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{BASE_IMAGE_ID}:{VERSION}\n",
    "! docker push {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{BASE_IMAGE_ID}:{VERSION}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
