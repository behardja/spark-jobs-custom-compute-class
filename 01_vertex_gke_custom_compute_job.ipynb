{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ece3b04-5703-4f75-88e8-6d6565e694d0",
   "metadata": {},
   "source": [
    "# GKE workloads and executions using Vertex AI \n",
    "\n",
    "## Objective\n",
    "Execute a Vertex Custom job that runs a Spark workload on a GKE cluster that uses custom compute classes. Users would define ComputeClass with a list of resource preferences. GKE would attempt to fulfill resources according to this list (e.g. L4 > T4 > CPU), and when a preferred resource is unavailable, a fallback strategy would shift to the next suitable resource.\n",
    "\n",
    "## Flow Diagram\n",
    "![image_png2.PNG](./img/vertex_gke_flow.PNG)<br>\n",
    "\n",
    "### Work Flow Pattern\n",
    "- GKE cluster (Standard Mode, Autopilot) is created and a Custom Compute class is set as the default for a namespace \n",
    "- Vertex Custom Job pulls and submits containerized workloads from Artifact Registry using WorkerPoolSpecs\n",
    "- Spark workload is run in the Kubernetes cluster specified in configuration\n",
    "\n",
    "\n",
    "## Google Cloud services and resources:\n",
    "\n",
    "- `Vertex AI`\n",
    "- `Artifact Registry`\n",
    "- `Cloud Storage`\n",
    "- `Kubernetes Engine`\n",
    "- `Compute Engine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1e51d-c007-49cc-8a35-b0dac5e94ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the versions of the packages installed\n",
    "\n",
    "! kubectl version --client\n",
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c3e8ec47-b560-47e0-bba3-b3a99592010c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project parameters\n",
    "PROJECT_ID = \"sandbox-401718\" # @param {type:\"string\"}\n",
    "REGION=\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "# Cluster parameters\n",
    "NETWORK=\"beusebio-network\" # @param {type:\"string\"}\n",
    "cluster_name = \"ccc-test-region-autopilot\" # @param {type:\"string\"}\n",
    "cluster_zone = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "# storage bucket to store intermediate artifacts such as YAML job files\n",
    "BUCKET_URI = \"gs://sandbox-401718-us-notebooks/gke-yaml\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "d0216a03-4eb7-4f70-a9fb-9070f9655d57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster ccc-test-region-autopilot in us-central1... Cluster is being c\n",
      "onfigured...⠼                                                                  \n",
      "Creating cluster ccc-test-region-autopilot in us-central1... Cluster is being d\n",
      "eployed...⠶                                                                    \n",
      "Creating cluster ccc-test-region-autopilot in us-central1... Cluster is being h\n",
      "ealth-checked (Kubernetes Control Plane is healthy)...done.                    \n",
      "Created [https://container.googleapis.com/v1/projects/sandbox-401718/zones/us-central1/clusters/ccc-test-region-autopilot].\n",
      "To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1/ccc-test-region-autopilot?project=sandbox-401718\n",
      "kubeconfig entry generated for ccc-test-region-autopilot.\n",
      "NAME                       LOCATION     MASTER_VERSION      MASTER_IP      MACHINE_TYPE  NODE_VERSION        NUM_NODES  STATUS\n",
      "ccc-test-region-autopilot  us-central1  1.31.4-gke.1256000  34.173.27.183  e2-small      1.31.4-gke.1256000  3          RUNNING\n"
     ]
    }
   ],
   "source": [
    "! gcloud container clusters create-auto {cluster_name} \\\n",
    "    --network={NETWORK} \\\n",
    "    --location=us-central1 \\\n",
    "    --release-channel=regular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557c38a-8841-4a75-8a2d-e376b8cfa78d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set and connect to the Kubernetes Master Server IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "18ba760e-e58d-48db-bbbc-38cdeb2c8a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for ccc-test-region-autopilot.\n"
     ]
    }
   ],
   "source": [
    "K8S = \"https://34.173.27.183\" # @param {type:\"string\"}\n",
    "\n",
    "! gcloud container clusters get-credentials {cluster_name} --location {cluster_zone} --project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a1cb9-90e3-4dda-b493-c85115ccf033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud container clusters describe {cluster_name} --location {cluster_zone}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c373a39-8b88-460d-b1ae-dd3cc2d9a91f",
   "metadata": {},
   "source": [
    "### Define a Custom Compule Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "90d7137b-80fe-4934-a2d4-9feca1d51cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/computeclass.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/computeclass.yaml\n",
    "\n",
    "apiVersion: cloud.google.com/v1\n",
    "kind: ComputeClass\n",
    "metadata:\n",
    "  name: l4-t4-cpu\n",
    "spec:\n",
    "  priorities:\n",
    "  - gpu:\n",
    "      count: 1\n",
    "      type: nvidia-l4\n",
    "  - gpu:\n",
    "      count: 1\n",
    "      type: nvidia-tesla-t4\n",
    "  - machineFamily: n1\n",
    "    minCores: 16\n",
    "  activeMigration:\n",
    "    optimizeRulePriority: true\n",
    "  nodePoolAutoCreation:\n",
    "    enabled: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "de2aefcd-6fb7-48d7-b84f-99afa18f75ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computeclass.cloud.google.com/l4-t4-cpu created\n"
     ]
    }
   ],
   "source": [
    "# Apply compute class\n",
    "! kubectl apply -f ./src/computeclass.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "6184dfd3-3689-4e4f-81c0-4521ffbbdddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         l4-t4-cpu\n",
      "Namespace:    \n",
      "Labels:       <none>\n",
      "Annotations:  <none>\n",
      "API Version:  cloud.google.com/v1\n",
      "Kind:         ComputeClass\n",
      "Metadata:\n",
      "  Creation Timestamp:  2025-02-07T00:28:02Z\n",
      "  Generation:          1\n",
      "  Managed Fields:\n",
      "    API Version:  cloud.google.com/v1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:metadata:\n",
      "        f:annotations:\n",
      "          .:\n",
      "          f:kubectl.kubernetes.io/last-applied-configuration:\n",
      "      f:spec:\n",
      "        .:\n",
      "        f:activeMigration:\n",
      "          .:\n",
      "          f:optimizeRulePriority:\n",
      "        f:nodePoolAutoCreation:\n",
      "          .:\n",
      "          f:enabled:\n",
      "        f:priorities:\n",
      "        f:whenUnsatisfiable:\n",
      "    Manager:         kubectl-client-side-apply\n",
      "    Operation:       Update\n",
      "    Time:            2025-02-07T00:28:02Z\n",
      "  Resource Version:  10366\n",
      "  UID:               4334af08-1960-4cec-8e23-c334a57625bf\n",
      "Spec:\n",
      "  Active Migration:\n",
      "    Optimize Rule Priority:  true\n",
      "  Node Pool Auto Creation:\n",
      "    Enabled:  true\n",
      "  Priorities:\n",
      "    Gpu:\n",
      "      Count:           1\n",
      "      Driver Version:  default\n",
      "      Type:            nvidia-l4\n",
      "    Gpu:\n",
      "      Count:           1\n",
      "      Driver Version:  default\n",
      "      Type:            nvidia-tesla-t4\n",
      "    Machine Family:    n1\n",
      "    Min Cores:         16\n",
      "  When Unsatisfiable:  ScaleUpAnyway\n",
      "Events:                <none>\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe computeclass l4-t4-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e8ccf-ffd8-42e8-83d2-fbd2523f370a",
   "metadata": {},
   "source": [
    "### Test Example Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "51aa19bb-9e11-4d15-a1e5-d723dbc72a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/workload.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/workload.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: custom-workload\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: custom-workload\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: custom-workload\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        cloud.google.com/compute-class: l4-t4-cpu\n",
    "      containers:\n",
    "      - name: test\n",
    "        image: gcr.io/google_containers/pause\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 1.5\n",
    "            memory: \"4Gi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8e450a8c-3d68-4449-b54b-e40e1ae29bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/custom-workload created\n"
     ]
    }
   ],
   "source": [
    "# Apply compute class\n",
    "! kubectl apply -f ./src/workload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "6c30bbf5-7956-468b-8ec7-d50cf142b6de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Gives detailed information about the  Deployment\n",
    "! kubectl describe deployment custom-workload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "673ecba7-49c4-497b-9803-ed12ee9c3771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              READY   STATUS    RESTARTS   AGE\n",
      "custom-workload-79dd44d75-f2gbd   1/1     Running   0          106s\n",
      "custom-workload-79dd44d75-lf99j   1/1     Running   0          106s\n"
     ]
    }
   ],
   "source": [
    "# Check that all Pods are running\n",
    "! kubectl get pods -l=app=custom-workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ec2ca6c0-c0bc-4cf2-ae04-a5b479f718c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                               STATUS   ROLES    AGE   VERSION\n",
      "gke-ccc-test-autoprov-default-pool-f68a1614-f9wh   Ready    <none>   3m    v1.31.4-gke.1256000\n",
      "gke-ccc-test-autoprov-default-pool-f68a1614-hv5t   Ready    <none>   3m    v1.31.4-gke.1256000\n",
      "gke-ccc-test-autoprov-default-pool-f68a1614-wbx8   Ready    <none>   3m    v1.31.4-gke.1256000\n"
     ]
    }
   ],
   "source": [
    "# View nodes\n",
    "! kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9eb00-2d4a-4b81-b27a-572fc9af66c8",
   "metadata": {},
   "source": [
    "## Spark on GPU-enabled Kubernetes\n",
    "\n",
    "Build image to run and submit Apache Spark applications on Kubernetes. Steps include downloading files from Nvidia and Spark into a local `src/` folder. In this example, no operators are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed8af6-4fef-4640-920f-f788e432b1e3",
   "metadata": {},
   "source": [
    "### Configure RBAC Role\n",
    "Create namespace, configure user control for managing access to Kubernetes cluster resources, and verify permissions to run Spark workloads on Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "7135d6f5-c625-4e2a-b2cc-5a53345340a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/spark-role.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/spark-role.yaml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: spark-demo\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: spark\n",
    "  namespace: spark-demo\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: spark-role\n",
    "  namespace: spark-demo\n",
    "subjects:\n",
    "  - kind: ServiceAccount\n",
    "    name: spark\n",
    "    namespace: spark-demo\n",
    "roleRef:\n",
    "  kind: ClusterRole\n",
    "  name: edit\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e71811a1-8ec1-41cb-a358-35542be842f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create namespace, apply RBAC cofig, Custom Compute Class as default, and verify permissions to run Spark workloads on Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "a10e4f9d-ac9f-43a3-8a1e-27bfc911ec9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/spark-demo created\n",
      "namespace/spark-demo labeled\n"
     ]
    }
   ],
   "source": [
    "# Create namespace, apply RBAC cofig, and verify permissions to run Spark workloads on Kubernetes\n",
    "! kubectl create namespace spark-demo\n",
    "! kubectl label namespaces spark-demo \\\n",
    "    cloud.google.com/default-compute-class=l4-t4-cpu\n",
    "! ! kubectl --namespace=spark-demo apply -f ./src/spark-role.yaml\n",
    "! kubectl auth can-i create pod --namespace spark --as=system:serviceaccount:spark-demo:spark\n",
    "! kubectl auth can-i delete services --namespace spark --as=system:serviceaccount:spark-demo:spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ad1b6-5c12-443b-a699-fab43839a98c",
   "metadata": {},
   "source": [
    "### Spark Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e0506369-dc7a-4a02-a0f4-6897a3670a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "VERSION=\"latest\"\n",
    "REPO_NAME=\"gke-mlops-pilot-docker\" # @param {type:\"string\"}\n",
    "JOB_IMAGE_ID=\"spark-gke\" # @param {type:\"string\"}\n",
    "BASE_IMAGE_ID = \"component-base\" # @param {type:\"string\"}\n",
    "\n",
    "# Vertex Custom Job parameters\n",
    "SERVICE_ACCOUNT=\"757654702990-compute@developer.gserviceaccount.com\" # @param {type:\"string\"}\n",
    "PIPELINE_ROOT=\"gs://sanbox-bucket-kfp-intro-demo\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "f68f8326-8b75-4006-8f83-3cb9556e99a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "8d7e807d-0e73-4d61-a824-5da5efed044e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sprk Pi test\n",
    "\n",
    "CMD = [\n",
    "    r\"\"\"gcloud container clusters get-credentials {cluster_name_} --zone {cluster_zone_} --project {project} &&./bin/spark-submit \\\n",
    "        --master k8s://{k8s} \\\n",
    "        --deploy-mode cluster \\\n",
    "        --name spark-pi \\\n",
    "        --class org.apache.spark.examples.SparkPi \\\n",
    "        --conf spark.kubernetes.driver.request.cores=400m \\\n",
    "        --conf spark.kubernetes.executor.request.cores=100m \\\n",
    "        --conf spark.kubernetes.container.image={image} \\\n",
    "        --conf spark.kubernetes.namespace=spark-demo \\\n",
    "        --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n",
    "        local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar\"\"\".format(\n",
    "        cluster_name_=cluster_name,\n",
    "        cluster_zone_=cluster_zone,\n",
    "        project=PROJECT_ID,\n",
    "        k8s=K8S,\n",
    "        image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "19df5ec3-a66e-476f-abdd-6bd73137de7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WORKER_POOL_SPEC_ = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\"machine_type\": \"n1-standard-4\", \"accelerator_count\": 0},\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{BASE_IMAGE_ID}:{VERSION}\",\n",
    "            \"command\": [\"sh\", \"-c\"],\n",
    "            \"args\": CMD\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "cce77315-37fe-4108-85a1-678f3e22c6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=\"k8s-custom-job\",\n",
    "    worker_pool_specs=WORKER_POOL_SPEC_,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "custom_job.run(sync=False, service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a104b1a-b9ef-497f-b422-dd5585f20aff",
   "metadata": {},
   "source": [
    "### Check Kubernetes Task Completion and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "927c6654-82ac-41f2-a375-63d925a402ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                               READY   STATUS      RESTARTS   AGE\n",
      "spark-pi-d68f3f94ddd6e3e4-driver   0/1     Completed   0          34m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods --namespace=spark-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "55e56e13-4fa7-46b2-a0d5-76f2e3048975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ id -u\n",
      "+ myuid=185\n",
      "++ id -g\n",
      "+ mygid=0\n",
      "+ set +e\n",
      "++ getent passwd 185\n",
      "+ uidentry=\n",
      "+ set -e\n",
      "+ '[' -z '' ']'\n",
      "+ '[' -w /etc/passwd ']'\n",
      "+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'\n",
      "+ '[' -z /usr/lib/jvm/java-1.8.0-openjdk-amd64 ']'\n",
      "+ SPARK_CLASSPATH=':/opt/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "++ command -v readarray\n",
      "+ '[' readarray ']'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*:/opt/spark/work-dir'\n",
      "+ case \"$1\" in\n",
      "+ shift 1\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --conf \"spark.executorEnv.SPARK_DRIVER_POD_IP=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\")\n",
      "+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.86.0.71 --conf spark.executorEnv.SPARK_DRIVER_POD_IP=10.86.0.71 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.SparkPi local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar\n",
      "Files local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar from /opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar to /opt/spark/work-dir/spark-examples_2.12-3.5.0.jar\n",
      "25/02/07 00:38:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/07 00:38:56 INFO SparkContext: Running Spark version 3.5.0\n",
      "25/02/07 00:38:56 INFO SparkContext: OS info Linux, 6.6.56+, amd64\n",
      "25/02/07 00:38:56 INFO SparkContext: Java version 1.8.0_382\n",
      "25/02/07 00:38:56 INFO ResourceUtils: ==============================================================\n",
      "25/02/07 00:38:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/02/07 00:38:56 INFO ResourceUtils: ==============================================================\n",
      "25/02/07 00:38:56 INFO SparkContext: Submitted application: Spark Pi\n",
      "25/02/07 00:38:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/02/07 00:38:57 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "25/02/07 00:38:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/02/07 00:38:57 INFO SecurityManager: Changing view acls to: 185,root\n",
      "25/02/07 00:38:57 INFO SecurityManager: Changing modify acls to: 185,root\n",
      "25/02/07 00:38:57 INFO SecurityManager: Changing view acls groups to: \n",
      "25/02/07 00:38:57 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/02/07 00:38:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: 185, root; groups with view permissions: EMPTY; users with modify permissions: 185, root; groups with modify permissions: EMPTY\n",
      "25/02/07 00:38:57 INFO Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "25/02/07 00:38:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/02/07 00:38:57 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/02/07 00:38:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/02/07 00:38:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/02/07 00:38:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/02/07 00:38:57 INFO DiskBlockManager: Created local directory at /var/data/spark-c8c3123b-4482-4223-9dd6-a4e15fd834ec/blockmgr-3ca11c32-b39a-4b97-a7ee-fb61fd88253f\n",
      "25/02/07 00:38:57 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "25/02/07 00:38:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/02/07 00:38:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/02/07 00:38:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/02/07 00:38:58 INFO SparkContext: Added JAR local:/opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar at file:/opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar with timestamp 1738888736880\n",
      "25/02/07 00:38:58 WARN SparkContext: The JAR local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar at file:/opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar has been added already. Overwriting of added jar is not supported in the current version.\n",
      "25/02/07 00:38:58 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "25/02/07 00:39:00 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 2, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
      "25/02/07 00:39:00 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs\n",
      "25/02/07 00:39:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "25/02/07 00:39:00 INFO NettyBlockTransferService: Server created on spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc 10.86.0.71:7079\n",
      "25/02/07 00:39:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/02/07 00:39:00 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "25/02/07 00:39:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc, 7079, None)\n",
      "25/02/07 00:39:00 INFO BlockManagerMasterEndpoint: Registering block manager spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc:7079 with 413.9 MiB RAM, BlockManagerId(driver, spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc, 7079, None)\n",
      "25/02/07 00:39:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc, 7079, None)\n",
      "25/02/07 00:39:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc, 7079, None)\n",
      "25/02/07 00:39:01 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "25/02/07 00:39:07 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.86.0.72:44890\n",
      "25/02/07 00:39:08 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.86.0.72:44900) with ID 1,  ResourceProfileId 0\n",
      "25/02/07 00:39:08 INFO BlockManagerMasterEndpoint: Registering block manager 10.86.0.72:38603 with 413.9 MiB RAM, BlockManagerId(1, 10.86.0.72, 38603, None)\n",
      "25/02/07 00:39:08 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.86.0.73:57096\n",
      "25/02/07 00:39:09 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.86.0.73:57110) with ID 2,  ResourceProfileId 0\n",
      "25/02/07 00:39:09 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "25/02/07 00:39:09 INFO BlockManagerMasterEndpoint: Registering block manager 10.86.0.73:38105 with 413.9 MiB RAM, BlockManagerId(2, 10.86.0.73, 38105, None)\n",
      "25/02/07 00:39:10 INFO SparkContext: Starting job: reduce at SparkPi.scala:38\n",
      "25/02/07 00:39:10 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 2 output partitions\n",
      "25/02/07 00:39:10 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
      "25/02/07 00:39:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/07 00:39:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/07 00:39:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
      "25/02/07 00:39:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 413.9 MiB)\n",
      "25/02/07 00:39:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 413.9 MiB)\n",
      "25/02/07 00:39:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc:7079 (size: 2.3 KiB, free: 413.9 MiB)\n",
      "25/02/07 00:39:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/07 00:39:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/02/07 00:39:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "25/02/07 00:39:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.86.0.73, executor 2, partition 0, PROCESS_LOCAL, 7945 bytes) \n",
      "25/02/07 00:39:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (10.86.0.72, executor 1, partition 1, PROCESS_LOCAL, 7945 bytes) \n",
      "25/02/07 00:39:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.86.0.73:38105 (size: 2.3 KiB, free: 413.9 MiB)\n",
      "25/02/07 00:39:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.86.0.72:38603 (size: 2.3 KiB, free: 413.9 MiB)\n",
      "25/02/07 00:39:11 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1001 ms on 10.86.0.72 (executor 1) (1/2)\n",
      "25/02/07 00:39:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1061 ms on 10.86.0.73 (executor 2) (2/2)\n",
      "25/02/07 00:39:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/02/07 00:39:11 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 1.415 s\n",
      "25/02/07 00:39:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/07 00:39:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/02/07 00:39:11 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.490524 s\n",
      "Pi is roughly 3.1339956699783498\n",
      "25/02/07 00:39:11 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "25/02/07 00:39:11 INFO SparkUI: Stopped Spark web UI at http://spark-pi-d68f3f94ddd6e3e4-driver-svc.spark-demo.svc:4040\n",
      "25/02/07 00:39:11 INFO KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "25/02/07 00:39:11 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "25/02/07 00:39:11 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n",
      "25/02/07 00:39:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/02/07 00:39:11 INFO MemoryStore: MemoryStore cleared\n",
      "25/02/07 00:39:11 INFO BlockManager: BlockManager stopped\n",
      "25/02/07 00:39:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/02/07 00:39:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/02/07 00:39:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "25/02/07 00:39:11 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/02/07 00:39:11 INFO ShutdownHookManager: Deleting directory /var/data/spark-c8c3123b-4482-4223-9dd6-a4e15fd834ec/spark-554005cd-51c6-4b88-a2cf-d957a391a302\n",
      "25/02/07 00:39:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e2927b-aa11-4bfc-8924-8305cfd9f7ae\n"
     ]
    }
   ],
   "source": [
    " # Check the logs for any Pod\n",
    "    \n",
    "pod = \"spark-pi-d68f3f94ddd6e3e4-driver\"    # @param {type:\"string\"}\n",
    "! kubectl logs {pod} --namespace=spark-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "913548b7-6cc7-4a5e-a19c-9f017b08a7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             spark-pi-d68f3f94ddd6e3e4-driver\n",
      "Namespace:        spark-demo\n",
      "Priority:         0\n",
      "Service Account:  spark\n",
      "Node:             gk3-ccc-test-region-auto-nap-1rlkbinn-a000bd62-hm6z/10.128.15.206\n",
      "Start Time:       Fri, 07 Feb 2025 00:37:23 +0000\n",
      "Labels:           spark-app-name=spark-pi\n",
      "                  spark-app-selector=spark-4709dc795e3747178f21befd37354f2b\n",
      "                  spark-role=driver\n",
      "                  spark-version=3.5.0\n",
      "Annotations:      autopilot.gke.io/resource-adjustment:\n",
      "                    {\"input\":{\"containers\":[{\"limits\":{\"memory\":\"1408Mi\"},\"requests\":{\"cpu\":\"400m\",\"memory\":\"1408Mi\"},\"name\":\"spark-kubernetes-driver\"}]},\"out...\n",
      "                  autopilot.gke.io/warden-version: 31.23.0-gke.7\n",
      "Status:           Succeeded\n",
      "IP:               10.86.0.71\n",
      "IPs:\n",
      "  IP:  10.86.0.71\n",
      "Containers:\n",
      "  spark-kubernetes-driver:\n",
      "    Container ID:  containerd://4e921f6493c8a0fa6daaf0ba79855aaef46df264b34e9df6aeb74a31f744f500\n",
      "    Image:         us-central1-docker.pkg.dev/sandbox-401718/gke-mlops-pilot-docker/spark-gke:latest\n",
      "    Image ID:      us-central1-docker.pkg.dev/sandbox-401718/gke-mlops-pilot-docker/spark-gke@sha256:016aa34170018507a6f2644022c71e265d1e2f03ef4e841ea71ca1284bc8e31f\n",
      "    Ports:         7078/TCP, 7079/TCP, 4040/TCP\n",
      "    Host Ports:    0/TCP, 0/TCP, 0/TCP\n",
      "    Args:\n",
      "      driver\n",
      "      --properties-file\n",
      "      /opt/spark/conf/spark.properties\n",
      "      --class\n",
      "      org.apache.spark.examples.SparkPi\n",
      "      local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Fri, 07 Feb 2025 00:38:48 +0000\n",
      "      Finished:     Fri, 07 Feb 2025 00:39:12 +0000\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             1408Mi\n",
      "    Requests:\n",
      "      cpu:                400m\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             1408Mi\n",
      "    Environment:\n",
      "      SPARK_USER:                 root\n",
      "      SPARK_APPLICATION_ID:       spark-4709dc795e3747178f21befd37354f2b\n",
      "      SPARK_DRIVER_BIND_ADDRESS:   (v1:status.podIP)\n",
      "      SPARK_LOCAL_DIRS:           /var/data/spark-c8c3123b-4482-4223-9dd6-a4e15fd834ec\n",
      "      SPARK_CONF_DIR:             /opt/spark/conf\n",
      "    Mounts:\n",
      "      /opt/spark/conf from spark-conf-volume-driver (rw)\n",
      "      /var/data/spark-c8c3123b-4482-4223-9dd6-a4e15fd834ec from spark-local-dir-1 (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-824gm (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   False \n",
      "  Initialized                 True \n",
      "  Ready                       False \n",
      "  ContainersReady             False \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  spark-local-dir-1:\n",
      "    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n",
      "    Medium:     \n",
      "    SizeLimit:  <unset>\n",
      "  spark-conf-volume-driver:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      spark-drv-19847794ddd6fe70-conf-map\n",
      "    Optional:  false\n",
      "  kube-api-access-824gm:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              cloud.google.com/compute-class=l4-t4-cpu\n",
      "Tolerations:                 cloud.google.com/compute-class=l4-t4-cpu:NoSchedule\n",
      "                             kubernetes.io/arch=amd64:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason       Age                From                                   Message\n",
      "  ----     ------       ----               ----                                   -------\n",
      "  Normal   Scheduled    37m                gke.io/optimize-utilization-scheduler  Successfully assigned spark-demo/spark-pi-d68f3f94ddd6e3e4-driver to gk3-ccc-test-region-auto-nap-1rlkbinn-a000bd62-hm6z\n",
      "  Warning  FailedMount  37m                kubelet                                MountVolume.SetUp failed for volume \"spark-conf-volume-driver\" : configmap \"spark-drv-19847794ddd6fe70-conf-map\" not found\n",
      "  Warning  Failed       37m                kubelet                                Failed to pull image \"us-central1-docker.pkg.dev/sandbox-401718/gke-mlops-pilot-docker/spark-gke:latest\": failed to pull and unpack image \"us-central1-docker.pkg.dev/sandbox-401718/gke-mlops-pilot-docker/spark-gke:latest\": failed to copy: read tcp 10.128.15.206:60134->209.85.200.82:443: read: connection reset by peer\n",
      "  Warning  Failed       37m                kubelet                                Error: ErrImagePull\n",
      "  Normal   BackOff      37m                kubelet                                Back-off pulling image \"us-central1-docker.pkg.dev/sandbox-401718/gke-mlops-pilot-docker/spark-gke:latest\"\n",
      "  Warning  Failed       37m                kubelet                                Error: ImagePullBackOff\n",
      "  Normal   Pulling      37m (x2 over 37m)  kubelet                                Pulling image \"us-central1-docker.pkg.dev/sandbox-401718/gke-mlops-pilot-docker/spark-gke:latest\"\n",
      "  Normal   Pulled       36m                kubelet                                Successfully pulled image \"us-central1-docker.pkg.dev/sandbox-401718/gke-mlops-pilot-docker/spark-gke:latest\" in 54.734s (54.734s including waiting). Image size: 5019135845 bytes.\n",
      "  Normal   Created      36m                kubelet                                Created container spark-kubernetes-driver\n",
      "  Normal   Started      36m                kubelet                                Started container spark-kubernetes-driver\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe pod spark-pi-d68f3f94ddd6e3e4-driver --namespace=spark-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ef08e868-3ec9-4dd8-ac31-7305a03f8306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                  STATUS   ROLES    AGE   VERSION\n",
      "gk3-ccc-test-region-auto-nap-1rlkbinn-a000bd62-hm6z   Ready    <none>   92m   v1.31.4-gke.1256000\n"
     ]
    }
   ],
   "source": [
    "! kubectl get nodes -l cloud.google.com/compute-class=l4-t4-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ae876e2c-bdea-4f64-84f9-49a23ea059e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"custom-workload\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete Cluster\n",
    "! gcloud container clusters delete {cluster_name} --zone {cluster_zone} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c864be1-4254-4814-b622-0077e613506a",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "* [About Custom Compute Classes](https://cloud.google.com/kubernetes-engine/docs/concepts/about-custom-compute-classes)\n",
    "* [Running Spark on Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)\n",
    "* [Getting Started with RAPIDS and Kubernetes](https://docs.nvidia.com/ai-enterprise/deployment-guide-spark-rapids-accelerator/0.1.0/kubernetes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003527d3-048a-4589-8ed5-063518f9608b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
