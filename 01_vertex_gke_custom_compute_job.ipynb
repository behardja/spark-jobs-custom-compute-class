{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ece3b04-5703-4f75-88e8-6d6565e694d0",
   "metadata": {},
   "source": [
    "# Spark workloads Executed using GKE Custom Compute Classes\n",
    "\n",
    "## Objective\n",
    "Execute a Spark workload on a GKE cluster that uses custom compute classes. Jobs are submitted via spark-submit layer through Vertex Custom jobs. Users would define `ComputeClass.yaml` with a list of resource preferences. GKE would attempt to fulfill resources according to this list (e.g. L4 > T4 > CPU), and when a preferred resource is unavailable, a fallback strategy would shift to the next suitable resource. Custom Compute Class is set as the default for the namespace that runs the spark workload.\n",
    "\n",
    "\n",
    "### Work Flow\n",
    "- GKE cluster (Autopilot mode) is created and a Custom Compute class is set as the default for a namespace \n",
    "- Vertex Custom Job pulls and submits containerized workloads from Artifact Registry using WorkerPoolSpecs\n",
    "- Spark workload is run in the Kubernetes cluster specified in configuration\n",
    "\n",
    "\n",
    "## Google Cloud services and resources:\n",
    "\n",
    "- `Vertex AI`\n",
    "- `Artifact Registry`\n",
    "- `Cloud Storage`\n",
    "- `Kubernetes Engine`\n",
    "- `Compute Engine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1e51d-c007-49cc-8a35-b0dac5e94ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the versions of the packages installed\n",
    "\n",
    "! kubectl version --client\n",
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8ec47-b560-47e0-bba3-b3a99592010c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project parameters\n",
    "PROJECT_ID = \"sandbox-401718\" # @param {type:\"string\"}\n",
    "REGION=\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "# Cluster parameters\n",
    "NETWORK=\"beusebio-network\" # @param {type:\"string\"}\n",
    "cluster_name = \"ccc-test-region-autopilot\" # @param {type:\"string\"}\n",
    "cluster_zone = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "# storage bucket to store intermediate artifacts such as YAML job files\n",
    "BUCKET_URI = \"gs://sandbox-401718-us-notebooks/gke-yaml\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557c38a-8841-4a75-8a2d-e376b8cfa78d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Cluster\n",
    "\n",
    "Custom Compute Classes work with GKE Autopilot Clusters as well as GKE Standard Clusters with Autoprovisioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0216a03-4eb7-4f70-a9fb-9070f9655d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud container clusters create-auto {cluster_name} \\\n",
    "    --network={NETWORK} \\\n",
    "    --location=us-central1 \\\n",
    "    --release-channel=regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba760e-e58d-48db-bbbc-38cdeb2c8a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set and connect to the Kubernetes Master Server IP address\n",
    "K8S = \"https://34.173.27.183\" # @param {type:\"string\"}\n",
    "\n",
    "! gcloud container clusters get-credentials {cluster_name} --location {cluster_zone} --project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a1cb9-90e3-4dda-b493-c85115ccf033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud container clusters describe {cluster_name} --location {cluster_zone}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c373a39-8b88-460d-b1ae-dd3cc2d9a91f",
   "metadata": {},
   "source": [
    "### Define a Custom Compule Class\n",
    "\n",
    "Custom compute classes control the properties of the nodes that Google Kubernetes Engine (GKE) provisions when autoscaling your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7137b-80fe-4934-a2d4-9feca1d51cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/computeclass.yaml\n",
    "\n",
    "apiVersion: cloud.google.com/v1\n",
    "kind: ComputeClass\n",
    "metadata:\n",
    "  name: l4-t4-cpu\n",
    "spec:\n",
    "  priorities:\n",
    "  - gpu:\n",
    "      count: 1\n",
    "      type: nvidia-l4\n",
    "  - gpu:\n",
    "      count: 1\n",
    "      type: nvidia-tesla-t4\n",
    "  - machineFamily: n1\n",
    "    minCores: 16\n",
    "  activeMigration:\n",
    "    optimizeRulePriority: true\n",
    "  nodePoolAutoCreation:\n",
    "    enabled: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2aefcd-6fb7-48d7-b84f-99afa18f75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply compute class\n",
    "! kubectl apply -f ./src/computeclass.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184dfd3-3689-4e4f-81c0-4521ffbbdddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl describe computeclass l4-t4-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e8ccf-ffd8-42e8-83d2-fbd2523f370a",
   "metadata": {},
   "source": [
    "### Test Example Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa19bb-9e11-4d15-a1e5-d723dbc72a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/workload.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: custom-workload\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: custom-workload\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: custom-workload\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        cloud.google.com/compute-class: l4-t4-cpu\n",
    "      containers:\n",
    "      - name: test\n",
    "        image: gcr.io/google_containers/pause\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 1.5\n",
    "            memory: \"4Gi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e450a8c-3d68-4449-b54b-e40e1ae29bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply compute class\n",
    "! kubectl apply -f ./src/workload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30bbf5-7956-468b-8ec7-d50cf142b6de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Gives detailed information about the  Deployment\n",
    "! kubectl describe deployment custom-workload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ecba7-49c4-497b-9803-ed12ee9c3771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check that all Pods are running\n",
    "! kubectl get pods -l=app=custom-workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ca6c0-c0bc-4cf2-ae04-a5b479f718c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View nodes\n",
    "! kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9eb00-2d4a-4b81-b27a-572fc9af66c8",
   "metadata": {},
   "source": [
    "## Spark on GPU-enabled Kubernetes\n",
    "\n",
    "Build image to run and submit Apache Spark applications on Kubernetes. Steps include downloading files from Nvidia and Spark into a local `src/` folder. In this example, no operators are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed8af6-4fef-4640-920f-f788e432b1e3",
   "metadata": {},
   "source": [
    "### Configure RBAC Role\n",
    "Create namespace, configure user control for managing access to Kubernetes cluster resources, and verify permissions to run Spark workloads on Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135d6f5-c625-4e2a-b2cc-5a53345340a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/spark-role.yaml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: spark-demo\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: spark\n",
    "  namespace: spark-demo\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: spark-role\n",
    "  namespace: spark-demo\n",
    "subjects:\n",
    "  - kind: ServiceAccount\n",
    "    name: spark\n",
    "    namespace: spark-demo\n",
    "roleRef:\n",
    "  kind: ClusterRole\n",
    "  name: edit\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e4f9d-ac9f-43a3-8a1e-27bfc911ec9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create namespace, apply RBAC cofig, Custom Compute Class as default, and verify permissions to run Spark workloads on Kubernetes\n",
    "! kubectl create namespace spark-demo\n",
    "! kubectl label namespaces spark-demo \\\n",
    "    cloud.google.com/default-compute-class=l4-t4-cpu\n",
    "! ! kubectl --namespace=spark-demo apply -f ./src/spark-role.yaml\n",
    "! kubectl auth can-i create pod --namespace spark --as=system:serviceaccount:spark-demo:spark\n",
    "! kubectl auth can-i delete services --namespace spark --as=system:serviceaccount:spark-demo:spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ad1b6-5c12-443b-a699-fab43839a98c",
   "metadata": {},
   "source": [
    "### Spark Workload\n",
    "\n",
    "A Spark test job is run on the GKE cluster through a Vertex AI Custom Jobs. The Vertex Custom Job with a worker pool specification points to a pre-built Docker image containing spark-rapids, and allows users to submit spark jobs without using Kubernetes Operators for Spark. The Custom Compute Class is set as default for the spark-demo namespace.\n",
    "\n",
    "Set up required parameters.\n",
    "\n",
    "Container Image (created in *00_build_spark_images.ipynb*):\n",
    "- `VERSION`: version or tag of the Docker image. Default set as `latest`\n",
    "- `REPO_NAME`: The name of the Artifact Registry repository that will store the compiled pipeline file\n",
    "- `JOB_IMAGE_ID`: The name of the image that will be used to run spark jobs on Kubernetes. The full image name: `<REGION>-docker.pkg.dev/<PROJECT_ID>/<REPO_NAME>/<JOB_IMAGE_ID>:<VERSION>`\n",
    "- `BASE_IMAGE_ID`: The name of the image that will be used to submit jobs using Vertex AI. The full image name: `<REGION>-docker.pkg.dev/<PROJECT_ID>/<REPO_NAME>/<BASE_IMAGE_ID>:<VERSION>`\n",
    "<br>\n",
    "\n",
    "Custom Job:\n",
    "- `SERVICE_ACCOUNT`: The service account to use to run custom jobs and pipeline\n",
    "\n",
    "The final local `/src` folder will include the following: Dockerfile.cuda, spark (folder), getGpusResources.sh, rapids-4-spark_2.12-23.02.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0506369-dc7a-4a02-a0f4-6897a3670a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "VERSION=\"latest\"\n",
    "REPO_NAME=\"gke-mlops-pilot-docker\" # @param {type:\"string\"}\n",
    "JOB_IMAGE_ID=\"spark-gke\" # @param {type:\"string\"}\n",
    "BASE_IMAGE_ID = \"component-base\" # @param {type:\"string\"}\n",
    "\n",
    "# Vertex Custom Job parameters\n",
    "SERVICE_ACCOUNT=\"757654702990-compute@developer.gserviceaccount.com\" # @param {type:\"string\"}\n",
    "PIPELINE_ROOT=\"gs://sanbox-bucket-kfp-intro-demo\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f8326-8b75-4006-8f83-3cb9556e99a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e807d-0e73-4d61-a824-5da5efed044e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sprk Pi test\n",
    "\n",
    "CMD = [\n",
    "    r\"\"\"gcloud container clusters get-credentials {cluster_name_} --zone {cluster_zone_} --project {project} &&./bin/spark-submit \\\n",
    "        --master k8s://{k8s} \\\n",
    "        --deploy-mode cluster \\\n",
    "        --name spark-pi \\\n",
    "        --class org.apache.spark.examples.SparkPi \\\n",
    "        --conf spark.kubernetes.driver.request.cores=400m \\\n",
    "        --conf spark.kubernetes.executor.request.cores=100m \\\n",
    "        --conf spark.kubernetes.container.image={image} \\\n",
    "        --conf spark.kubernetes.namespace=spark-demo \\\n",
    "        --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n",
    "        local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar\"\"\".format(\n",
    "        cluster_name_=cluster_name,\n",
    "        cluster_zone_=cluster_zone,\n",
    "        project=PROJECT_ID,\n",
    "        k8s=K8S,\n",
    "        image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df5ec3-a66e-476f-abdd-6bd73137de7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WORKER_POOL_SPEC_ = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\"machine_type\": \"n1-standard-4\", \"accelerator_count\": 0},\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{BASE_IMAGE_ID}:{VERSION}\",\n",
    "            \"command\": [\"sh\", \"-c\"],\n",
    "            \"args\": CMD\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce77315-37fe-4108-85a1-678f3e22c6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=\"k8s-custom-job\",\n",
    "    worker_pool_specs=WORKER_POOL_SPEC_,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "custom_job.run(sync=False, service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a104b1a-b9ef-497f-b422-dd5585f20aff",
   "metadata": {},
   "source": [
    "### Check Kubernetes Task Completion and Output\n",
    "\n",
    "View the jobs that have been submitted to the Kubernetes cluster. Once the job and/or pipeline is complete, check to see the output of the spark-pi job.\n",
    "\n",
    "Example: `Job 0 finished: reduce at SparkPi.scala:38, took 1.490524 s\n",
    "Pi is roughly 3.1339956699783498`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c6654-82ac-41f2-a375-63d925a402ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl get pods --namespace=spark-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e56e13-4fa7-46b2-a0d5-76f2e3048975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Check the logs for any Pod\n",
    "    \n",
    "pod = \"spark-pi-d68f3f94ddd6e3e4-driver\"    # @param {type:\"string\"}\n",
    "! kubectl logs {pod} --namespace=spark-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913548b7-6cc7-4a5e-a19c-9f017b08a7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl describe pod spark-pi-d68f3f94ddd6e3e4-driver --namespace=spark-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08e868-3ec9-4dd8-ac31-7305a03f8306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl get nodes -l cloud.google.com/compute-class=l4-t4-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae876e2c-bdea-4f64-84f9-49a23ea059e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete Cluster\n",
    "! gcloud container clusters delete {cluster_name} --zone {cluster_zone} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c864be1-4254-4814-b622-0077e613506a",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "* [About Custom Compute Classes](https://cloud.google.com/kubernetes-engine/docs/concepts/about-custom-compute-classes)\n",
    "* [Running Spark on Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)\n",
    "* [Getting Started with RAPIDS and Kubernetes](https://docs.nvidia.com/ai-enterprise/deployment-guide-spark-rapids-accelerator/0.1.0/kubernetes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bab82d-9cef-43ed-b8a5-73aad6aaf9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
